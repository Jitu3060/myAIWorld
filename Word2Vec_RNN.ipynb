{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.utils import simple_preprocess"
      ],
      "metadata": {
        "id": "xAJClAYVejW0"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"I love machine learning\",\n",
        "    \"Word2Vec is a great algorithm\",\n",
        "    \"Implementing word2vec is fun and educational\"\n",
        "]"
      ],
      "metadata": {
        "id": "NwKCjdsHe3Pl"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_preprocess('Word2Vec is a great algorithm')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C_8YzuxfNm2",
        "outputId": "a798a5eb-3f84-4d56-82c7-ae0f356fac68"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['word', 'vec', 'is', 'great', 'algorithm']"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "wm9BXWh1eg8H"
      },
      "outputs": [],
      "source": [
        "# Preprocessing the text\n",
        "processed_corpus = [simple_preprocess(doc) for doc in corpus]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uocNgQeqg_Fc",
        "outputId": "752a0dfc-90c7-4a66-e299-54de850be8da"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['love', 'machine', 'learning'],\n",
              " ['word', 'vec', 'is', 'great', 'algorithm'],\n",
              " ['implementing', 'word', 'vec', 'is', 'fun', 'and', 'educational']]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(sentences=processed_corpus, vector_size=100, window=2, min_count=1, workers=4)"
      ],
      "metadata": {
        "id": "R-bBi7IXfGdJ"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv['fun']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZR_mVyoJ3Qfu",
        "outputId": "2d68b13e-271e-41ff-d522-23d064d7f49e"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-8.7274825e-03,  2.1301615e-03, -8.7354420e-04, -9.3190884e-03,\n",
              "       -9.4281426e-03, -1.4107180e-03,  4.4324086e-03,  3.7040710e-03,\n",
              "       -6.4986930e-03, -6.8730675e-03, -4.9994122e-03, -2.2868442e-03,\n",
              "       -7.2502876e-03, -9.6033178e-03, -2.7436293e-03, -8.3628409e-03,\n",
              "       -6.0388758e-03, -5.6709289e-03, -2.3441375e-03, -1.7069972e-03,\n",
              "       -8.9569986e-03, -7.3519943e-04,  8.1525063e-03,  7.6904297e-03,\n",
              "       -7.2061159e-03, -3.6668312e-03,  3.1185520e-03, -9.5707225e-03,\n",
              "        1.4764392e-03,  6.5244664e-03,  5.7464195e-03, -8.7630618e-03,\n",
              "       -4.5171441e-03, -8.1401607e-03,  4.5956374e-05,  9.2636338e-03,\n",
              "        5.9733056e-03,  5.0673080e-03,  5.0610625e-03, -3.2429171e-03,\n",
              "        9.5521836e-03, -7.3564244e-03, -7.2703874e-03, -2.2653891e-03,\n",
              "       -7.7856064e-04, -3.2161034e-03, -5.9258583e-04,  7.4888230e-03,\n",
              "       -6.9751858e-04, -1.6249407e-03,  2.7443992e-03, -8.3591007e-03,\n",
              "        7.8558037e-03,  8.5361041e-03, -9.5840869e-03,  2.4462664e-03,\n",
              "        9.9049713e-03, -7.6658037e-03, -6.9669187e-03, -7.7365171e-03,\n",
              "        8.3959233e-03, -6.8133592e-04,  9.1444086e-03, -8.1582209e-03,\n",
              "        3.7430846e-03,  2.6350426e-03,  7.4271322e-04,  2.3276759e-03,\n",
              "       -7.4690939e-03, -9.3583735e-03,  2.3545765e-03,  6.1484552e-03,\n",
              "        7.9856887e-03,  5.7358947e-03, -7.7733636e-04,  8.3061643e-03,\n",
              "       -9.3363142e-03,  3.4061326e-03,  2.6675343e-04,  3.8572443e-03,\n",
              "        7.3857834e-03, -6.7251669e-03,  5.5844807e-03, -9.5222248e-03,\n",
              "       -8.0445886e-04, -8.6887367e-03, -5.0986730e-03,  9.2892265e-03,\n",
              "       -1.8582619e-03,  2.9144264e-03,  9.0712793e-03,  8.9381328e-03,\n",
              "       -8.2084350e-03, -3.0123137e-03,  9.8866057e-03,  5.1044310e-03,\n",
              "       -1.5880871e-03, -8.6920215e-03,  2.9615164e-03, -6.6758976e-03],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the vector for a word\n",
        "vector = model.wv['fun']\n",
        "print(len(vector))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKSj6XAQfCYW",
        "outputId": "bac77682-ad7c-4588-d10e-7960d969042c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.index_to_key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEJGEVMumiDj",
        "outputId": "50508a8f-4ce4-4a28-c409-00c5df95b9db"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['is',\n",
              " 'vec',\n",
              " 'word',\n",
              " 'educational',\n",
              " 'and',\n",
              " 'fun',\n",
              " 'implementing',\n",
              " 'algorithm',\n",
              " 'great',\n",
              " 'learning',\n",
              " 'machine',\n",
              " 'love']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Find similar words\n",
        "similar_words = model.wv.most_similar('fun')\n",
        "print(similar_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szTUuXZjgti0",
        "outputId": "e161972f-d32f-4a28-c0e3-d8e8375141d4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('love', 0.16694684326648712), ('and', 0.13887687027454376), ('educational', 0.13149002194404602), ('word', 0.06408979743719101), ('great', 0.06059185042977333), ('machine', 0.020000355318188667), ('implementing', 0.019154027104377747), ('vec', 0.009391160681843758), ('algorithm', -0.05774581432342529), ('is', -0.059874895960092545)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Doc2Vec"
      ],
      "metadata": {
        "id": "jx3J7T8-gvAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_corpus = [TaggedDocument(words=simple_preprocess(doc), tags=[str(i)]) for i, doc in enumerate(corpus)]"
      ],
      "metadata": {
        "id": "RnAXbaLugFWg"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "333jHvI8597x",
        "outputId": "3f245868-b475-4cba-fa54-14676b6b2ca9"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TaggedDocument(words=['love', 'machine', 'learning'], tags=['0']),\n",
              " TaggedDocument(words=['word', 'vec', 'is', 'great', 'algorithm'], tags=['1']),\n",
              " TaggedDocument(words=['implementing', 'word', 'vec', 'is', 'fun', 'and', 'educational'], tags=['2'])]"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for k in enumerate(['a','b','c']):\n",
        "#   print(k)"
      ],
      "metadata": {
        "id": "jfzwjfdi5D4p"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# {n:m for m,n in enumerate(['a','b','c'])}"
      ],
      "metadata": {
        "id": "EmQLc3HU5Q9v"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvxZxnvAhko5",
        "outputId": "ccff3006-c8b9-41ca-c226-cc9f89232769"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TaggedDocument(words=['love', 'machine', 'learning'], tags=['0']),\n",
              " TaggedDocument(words=['word', 'vec', 'is', 'great', 'algorithm'], tags=['1']),\n",
              " TaggedDocument(words=['implementing', 'word', 'vec', 'is', 'fun', 'and', 'educational'], tags=['2'])]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the Doc2Vec model\n",
        "mod = Doc2Vec(tagged_corpus, vector_size=100, window=2, min_count=1, workers=4, epochs=20)"
      ],
      "metadata": {
        "id": "pacVR6SqgP4h"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector = mod.dv['0']\n",
        "print(vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__ZFiT8ogZcZ",
        "outputId": "7477fc24-eca0-454b-b9b8-3f0c3bdbde42"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.00527819 -0.00601931 -0.00989545  0.00857172  0.00359562  0.00025288\n",
            " -0.00988777 -0.00518546 -0.00973477  0.00202757  0.00281696  0.0046634\n",
            " -0.00433183 -0.00317033 -0.00306658 -0.00873361  0.00216362  0.0092466\n",
            " -0.00952865 -0.00346199 -0.0037968   0.00259918 -0.00569514  0.00265455\n",
            "  0.00579866 -0.00812233 -0.00836511 -0.0099757   0.00493463 -0.00914691\n",
            "  0.00585652  0.00680242 -0.00650747 -0.00455145 -0.00126637  0.00166305\n",
            " -0.00150028 -0.00857375 -0.00361711  0.00172257 -0.00202982 -0.00723293\n",
            "  0.00421299 -0.00860138  0.00270574 -0.00462646  0.00064875 -0.00203858\n",
            "  0.00541401 -0.00805321 -0.00214881 -0.00010938 -0.00666505 -0.00655978\n",
            " -0.00195023  0.00885407 -0.00125152  0.00356975 -0.0057673   0.00884441\n",
            "  0.00294264  0.00933568  0.00438536 -0.00421386  0.00224783 -0.00441896\n",
            "  0.0058108   0.00185461 -0.00227713 -0.0058888  -0.00807288 -0.00085071\n",
            " -0.00896562 -0.00923251 -0.00794214  0.00217329 -0.00653946 -0.00782066\n",
            "  0.00212299  0.00204655  0.00836188  0.00470029 -0.00944099 -0.00034617\n",
            "  0.00785754  0.00270869  0.00268829 -0.00490531  0.00647316  0.00165977\n",
            " -0.00764902  0.00688044 -0.00981634 -0.00817642 -0.00487046  0.00998701\n",
            "  0.00312241 -0.00202722  0.00894507  0.00234663]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find similar documents\n",
        "mod.dv.most_similar('1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9p-29wCAg67A",
        "outputId": "751b2fe5-1895-441c-b3ff-8f07352c48f8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('0', 0.164995938539505), ('2', -0.043281614780426025)]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mod.dv.index_to_key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycQ-3L9lh2em",
        "outputId": "e4e91a4f-5c6a-42d8-a9b8-87302eefc4e1"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0', '1', '2']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "9LG6yaPFrZ4N"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "YiwVq68Tra6p"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating sequences of word embeddings\n",
        "sequences = []\n",
        "for sentence in processed_corpus:\n",
        "    embeddings = [model.wv[word] for word in sentence if word in model.wv]\n",
        "    sequences.append(torch.tensor(embeddings, dtype=torch.float))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwSvjMMTrc7x",
        "outputId": "b47ba236-ec5c-4776-e5dd-af007826073a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-42-a8577b935d3d>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  sequences.append(torch.tensor(embeddings, dtype=torch.float))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwdiXd0Dr3gk",
        "outputId": "15b261af-5483-4da1-f876-c03fc0aa58fd"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Padding sequences to the same length\n",
        "padded_sequences = nn.utils.rnn.pad_sequence(sequences, batch_first=True)"
      ],
      "metadata": {
        "id": "idgVon7rr1Mk"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_sequences.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5gKkIu4sAYb",
        "outputId": "79299e8a-7cbe-4fb8-a249-a4498d082790"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 7, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_lengths = torch.tensor([len(seq) for seq in sequences])"
      ],
      "metadata": {
        "id": "gAFE8uMosJWJ"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_lengths"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wc12b2ZCsMYX",
        "outputId": "2b2be64a-402e-4709-f4dc-c0edca3f2a83"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3, 5, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        packed_input = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
        "        packed_output, hidden = self.rnn(packed_input)\n",
        "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
        "        return self.fc(hidden.squeeze(0))"
      ],
      "metadata": {
        "id": "B-8CgqnosYsp"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 100  # Same as the vector_size of Word2Vec\n",
        "hidden_size = 50\n",
        "output_size = 2  # Example output size (e.g., binary classification)"
      ],
      "metadata": {
        "id": "T5xVUGIutbkH"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNNModel(input_size, hidden_size, output_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "w1hLGE44smZV"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TensorDataset(padded_sequences, sequence_lengths)\n",
        "data_loader = DataLoader(dataset, batch_size=2, shuffle=True)"
      ],
      "metadata": {
        "id": "nB4EAloLtGkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for epoch in range(10):\n",
        "    for batch in data_loader:\n",
        "        padded_sequences, sequence_lengths = batch\n",
        "        labels = torch.randint(0, output_size, (padded_sequences.size(0),))  # Dummy labels for example purposes\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(padded_sequences, sequence_lengths)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "-uDaP6RusqdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Sample text data\n",
        "corpus = [\n",
        "    \"I love machine learning\",\n",
        "    \"Word2Vec is a great algorithm\",\n",
        "    \"Implementing word2vec is fun and educational\"\n",
        "]\n",
        "\n",
        "# Preprocessing the text\n",
        "processed_corpus = [simple_preprocess(doc) for doc in corpus]\n",
        "\n",
        "# Training the Word2Vec model using CBOW\n",
        "w2v_model = Word2Vec(sentences=processed_corpus, vector_size=100, window=4, min_count=1, workers=4, sg=0)\n",
        "\n",
        "# Creating sequences of word embeddings for next word prediction\n",
        "sequences = []\n",
        "targets = []\n",
        "for sentence in processed_corpus:\n",
        "    embeddings = [w2v_model.wv[word] for word in sentence if word in w2v_model.wv]\n",
        "    for i in range(len(embeddings) - 1):\n",
        "        sequences.append(torch.tensor(embeddings[:i+1], dtype=torch.float))\n",
        "        targets.append(torch.tensor(embeddings[i+1], dtype=torch.float))\n",
        "\n",
        "# Padding sequences to the same length\n",
        "padded_sequences = nn.utils.rnn.pad_sequence(sequences, batch_first=True)\n",
        "targets = torch.stack(targets)\n",
        "\n",
        "# Create DataLoader\n",
        "dataset = TensorDataset(padded_sequences, targets)\n",
        "data_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# Define the RNN Model for next word prediction\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, input_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        packed_input = nn.utils.rnn.pack_padded_sequence(x, x.size(1), batch_first=True, enforce_sorted=False)\n",
        "        packed_output, hidden = self.rnn(packed_input)\n",
        "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
        "        return self.fc(output[:, -1, :])\n",
        "\n",
        "# Initialize the RNN Model\n",
        "input_size = 100  # Same as the vector_size of Word2Vec\n",
        "hidden_size = 50\n",
        "\n",
        "model = RNNModel(input_size, hidden_size)\n",
        "criterion = nn.MSELoss()  # Mean Squared Error Loss for regression\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training the RNN Model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in data_loader:\n",
        "        padded_sequences, targets = batch\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(padded_sequences)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "# Example: Predict the next word embedding for a given sequence\n",
        "with torch.no_grad():\n",
        "    input_sequence = torch.tensor([w2v_model.wv[word] for word in simple_preprocess(\"I love machine\")], dtype=torch.float).unsqueeze(0)\n",
        "    padded_input_sequence = nn.utils.rnn.pad_sequence([input_sequence], batch_first=True)\n",
        "    next_word_embedding = model(padded_input_sequence)\n",
        "    print(next_word_embedding)\n"
      ],
      "metadata": {
        "id": "PzOcsEVGukBl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}